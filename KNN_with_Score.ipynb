{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f608ca-96c5-44fe-9b5d-40f03b02feb9",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classification\n",
    "\n",
    "This is a user-friendly Jupyter Notebook version of KNN classifier. It allows users to:\n",
    "\n",
    "- Upload a dataset\n",
    "- Choose parameters for K-Nearest Neighbors (KNN)\n",
    "- Select cross validation methods\n",
    "- View classification results (accuracy, sensitivity, specificity)\n",
    "- Visualize the confusion matrix\n",
    "- Visualizw score report for each sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18962687-0b97-4510-834c-97f2ae2fc109",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b199124c-f210-4a6d-b08e-1762c8e9a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np # Numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.model_selection import LeaveOneOut, StratifiedKFold  # Cross-validation strategies\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score  # Evaluation metrics\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, Dropdown, Button, HBox, VBox, Label, Layout   # For UI Control\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import io  # Provides tools for handling input/output\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Clear previous output\n",
    "output = widgets.Output()\n",
    "output.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af99286-8da1-4ac6-add0-0b3c4e25ff0a",
   "metadata": {},
   "source": [
    "## Interactive UI\n",
    "* Click the \"Upload File\" button and choose the input .csv file.\n",
    "* Then choose parameters for KNN and cross-validation methods\n",
    "  1. Pick a value for K\n",
    "  2. Select a distance metric method\n",
    "  3. Choose a weight scheme on how the items in nearest neighbors are weighted\n",
    "  4. Choose a cross validation method (leave-one-out or K-fold). If K-fold is chosen, select the number of folds.\n",
    "* Click the green button to run KNN and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dea6db9-a35f-437a-803c-efef807fe88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Import CSV File</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d980ad56d0a74a0e83ed18549061af64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.csv', description='Upload File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4fcb2154534aa4b9296419fb27988e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BoundedIntText(value=5, description='Number of nearest neighbors (K):', max=20, min=1, style=DescriptionStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7a3efad4dd45828619829bccc870a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Distance Metric:', options=('euclidean', 'manhattan', 'chebyshev'), style=DescriptionSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442e041724914db286bc9a88824947a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Weight Function:', options=('one', 'inverse', 'inversesquare'), style=DescriptionStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30da1ed21ec542a2bcdcbd57fda8898b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Cross Validation Method:', index=1, options=('Leave-One-Out', 'K-Fold'), style=Descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf146d08916b4bda92b1f87c926947e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c08b888caf499dbeec827d0386bb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Run KNN', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6146c611de45a9b9270f1f633ae650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input file widget\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.csv',\n",
    "    multiple=False,\n",
    "    description=\"Upload File\"\n",
    ")\n",
    "\n",
    "display(HTML(\"<b>Import CSV File</b>\"))\n",
    "display(uploader)\n",
    "\n",
    "# K (number of neighbors)\n",
    "k_widget = widgets.BoundedIntText(\n",
    "    value=5,                  # Default value\n",
    "    min=1, max=20, step=1,    # Allowed range and increment\n",
    "    description='Number of nearest neighbors (K):',  # Label shown next to the input\n",
    "    style={'description_width': 'initial'}  # Ensures the full description is visible (doesn't get truncated)\n",
    ")\n",
    "\n",
    "# Distance metric selection (Dropdown)\n",
    "metric_widget = widgets.Dropdown(\n",
    "    options=['euclidean', 'manhattan', 'chebyshev'],  # Available distance metrics\n",
    "    value='euclidean',              \n",
    "    description='Distance Metric:', \n",
    "    style={'description_width': 'initial'}  \n",
    ")\n",
    "\n",
    "# Weight function selection (Dropdown)\n",
    "weights_widget = widgets.Dropdown(\n",
    "    options=['one', 'inverse', 'inversesquare'],  # Custom weight options\n",
    "    value='one',                  \n",
    "    description='Weight Function:',  \n",
    "    style={'description_width': 'initial'}  \n",
    ")\n",
    "\n",
    "# Dropdown to select cross-validation method\n",
    "cv_widget = widgets.Dropdown(\n",
    "    options=['Leave-One-Out', 'K-Fold'],      # Available CV methods\n",
    "    value='K-Fold',                           \n",
    "    description='Cross Validation Method:',   \n",
    "    style={'description_width': 'initial'}    \n",
    ")\n",
    "\n",
    "# Numeric input for number of folds (only relevant for K-Fold)\n",
    "fold_widget = widgets.BoundedIntText(\n",
    "    value=5,           # Default value for folds\n",
    "    min=2,             # Minimum allowed number of folds\n",
    "    max=20,            # Maximum allowed number of folds\n",
    "    step=1,            # Increment step\n",
    "    description='Number of folds:',  # Label shown next to the input\n",
    "    style={'description_width': 'initial'}  # Prevent label truncation\n",
    ")\n",
    "\n",
    "# Output area for dynamically showing/hiding fold input\n",
    "cv_output = widgets.Output()\n",
    "\n",
    "# Function to dynamically show fold input only if 'K-Fold' is selected\n",
    "def update_cv_ui(change):\n",
    "    with cv_output:\n",
    "        cv_output.clear_output()  # Clear previous output\n",
    "        if cv_widget.value == 'K-Fold':\n",
    "            display(fold_widget)  # Show fold input only if 'K-Fold' is selected\n",
    "cv_widget.observe(update_cv_ui, names='value') # update_cv_ui() is run when cv_widget value changes\n",
    "\n",
    "display(k_widget,\n",
    "        metric_widget,\n",
    "        weights_widget\n",
    ")\n",
    "# Display the CV dropdown menu\n",
    "display(cv_widget)\n",
    "display(cv_output)\n",
    "# Manually call update function to set the initial state (since default is 'K-Fold')\n",
    "update_cv_ui(None)\n",
    "\n",
    "\n",
    "def run_knn(b):\n",
    "    with output:  # All printed output will appear inside the 'output' widget\n",
    "        output.clear_output()  # Clear previous output\n",
    "       \n",
    "        if uploader.value:\n",
    "            \n",
    "            uploaded_file = uploader.value[0]           # Get the first and only uploaded file\n",
    "            content = uploaded_file['content']          # Binary content of the file\n",
    "            file_name = uploaded_file['name']\n",
    "            \n",
    "            try:\n",
    "                data = pd.read_csv(io.BytesIO(content))     # Read into a DataFrame from memory buffer\n",
    "                X = data.drop(columns=['NAME', 'CLASS'])    # Use all columns except 'NAME' and 'CLASS' as features\n",
    "                y = data['CLASS']                           # Use 'CLASS' as the target label\n",
    "            except Exception as e:\n",
    "                # Catch and display any error that occurs during file read or processing\n",
    "                print(f\"❌ Error: {e}\")\n",
    "       \n",
    "            # Read hyperparameters and CV settings from widgets\n",
    "            k = k_widget.value                          # Number of neighbors\n",
    "            metric = metric_widget.value                # Distance metric\n",
    "            weights = weights_widget.value              # Weighting method\n",
    "            cv_method = cv_widget.value                 # Cross-validation method\n",
    "            n = 1 if cv_method == 'Leave-One-Out' else fold_widget.value  # LOOCV if 1 split, K-fold if otherwise\n",
    "\n",
    "            # Call the function with all collected inputs to run KNN model and evaluate\n",
    "            print(f\"\\n\\nRunning KNN with k={k}\")\n",
    "            print(f\"Cross-Validation was performed using {cv_method}\")\n",
    "            print(f\"Input file name: {file_name}\\n\")\n",
    "\n",
    "            evaluate_knn(X, y, k, metric, weights, n)\n",
    "        else:\n",
    "            print(\"⚠️ No file uploaded.\")\n",
    "\n",
    "# Run button\n",
    "run_button = widgets.Button(description='Run KNN', button_style='success')\n",
    "run_button.on_click(run_knn)\n",
    "\n",
    "# === Final Output Display ===\n",
    "display(\n",
    "    run_button,\n",
    "    output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da71a7-8ca3-4d9c-9347-4c4c2e5161b6",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "Below are helper functions for the notebook. If you have run all cells, you can skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42439631-e09c-48fc-9181-6ce48c5b84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized function for calculating weights\n",
    "def map_weights(weight):\n",
    "    if weight == 'one':\n",
    "        return 'uniform'\n",
    "    elif weight == 'inverse':\n",
    "        return 'distance'\n",
    "    else:\n",
    "        return lambda d: 1 / (d**2 + 1e-5)\n",
    "\n",
    "def get_weight_array(dists, k, weights):\n",
    "    if weights == \"one\":\n",
    "        return np.ones(k)\n",
    "    elif weights == \"inverse\":\n",
    "        return 1 / (dists + 1e-5)\n",
    "    else:\n",
    "        return 1 / (dists**2 + 1e-5)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88781c7b-4aa5-4cd6-bc90-0c136b2800b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall accuracy, sensitivity and selectivity for each class\n",
    "def evaluate_knn(X, y, k, metric, weights, n):\n",
    "    class_names = list(dict.fromkeys(y))       # This preserves the order of class names in the dataset\n",
    "    label_to_index = {label: idx for idx, label in enumerate(class_names)}\n",
    "    y_encoded = np.array([label_to_index[label] for label in y])            # Encode class labels as indices\n",
    "    class_indices = list(range(len(class_names)))        # A range of indices for class names (eg. 0,1,2)\n",
    "\n",
    "    # Get scores and predicted class labels for training and testing samples via cross validation\n",
    "    val_true, val_pred, val_scores, train_score_sums, train_score_counts, train_labels = run_cross_validation(\n",
    "        X, y_encoded, k, metric, weights, class_indices, n\n",
    "    )\n",
    "    train_score_avgs = train_score_sums / train_score_counts[:, np.newaxis]\n",
    "    \n",
    "    train_pred, train_true = [], []\n",
    "    train_pred = np.argmax(train_score_avgs, axis = 1)\n",
    "    train_true = [lbl for lbl in train_labels]           \n",
    "        \n",
    "    train_cm = confusion_matrix(train_true, train_pred, labels=class_indices)\n",
    "    val_cm = confusion_matrix(val_true, val_pred, labels=class_indices)      \n",
    "   \n",
    "    train_acc = accuracy_score(train_true, train_pred)\n",
    "    val_acc = accuracy_score(val_true, val_pred)\n",
    "    \n",
    "    display(HTML('<div style=\"text-align:left;\"><strong>Summary Report</strong></div>'))\n",
    "    print(f\"Training\\t{train_acc * 100:.2f}% ({int(train_acc * len(train_true))}/{len(train_true)})\")\n",
    "    print(f\"Validation\\t{val_acc * 100:.2f}% ({int(val_acc * len(val_true))}/{len(val_true)})\")\n",
    "\n",
    "    train_recall = recall_score(train_true, train_pred, labels=class_indices, average=None, zero_division=0)\n",
    "    train_specificity = compute_specificity(train_cm)\n",
    "\n",
    "    val_recall = recall_score(val_true, val_pred, labels=class_indices, average=None, zero_division=0)\n",
    "    val_specificity = compute_specificity(val_cm)\n",
    "\n",
    "    for i, cls in enumerate(class_names):  # Loop through each class\n",
    "        train_sens = train_recall[i] * 100  # Sensitivity = Recall\n",
    "        val_sens = val_recall[i] * 100\n",
    "        train_spec = train_specificity[i] * 100\n",
    "        val_spec = val_specificity[i] * 100\n",
    "        train_ner = (train_recall[i] + train_specificity[i]) / 2 * 100  # Non-error rate\n",
    "        val_ner = (val_recall[i] + val_specificity[i]) / 2 * 100\n",
    "\n",
    "        # Extract confusion matrix values\n",
    "        tp_train = train_cm[i][i]\n",
    "        fn_train = train_cm[i, :].sum() - tp_train\n",
    "        fp_train = train_cm[:, i].sum() - tp_train\n",
    "        tn_train = train_cm.sum() - (tp_train + fn_train + fp_train)\n",
    "\n",
    "        tp_val = val_cm[i][i]\n",
    "        fn_val = val_cm[i, :].sum() - tp_val\n",
    "        fp_val = val_cm[:, i].sum() - tp_val\n",
    "        tn_val = val_cm.sum() - (tp_val + fn_val + fp_val)\n",
    "\n",
    "        # Print table with metrics\n",
    "        print(\"\")\n",
    "        print(f\"{cls}\")\n",
    "        print(f\"{'':<10}{'Sensitivity':>20}{'Selectivity':>25}{'Non-Error Rate':>25}\")\n",
    "        print(f\"{'Training':<10}{train_sens:>14.2f}% ({tp_train}/{tp_train+fn_train}){train_spec:>17.2f}% ({tn_train}/{tn_train+fp_train}){train_ner:>21.2f}%\")\n",
    "        print(f\"{'Validation':<10}{val_sens:>14.2f}% ({tp_val}/{tp_val+fn_val}){val_spec:>17.2f}% ({tn_val}/{tn_val+fp_val}){val_ner:>21.2f}%\")\n",
    "\n",
    "    plot_confusion_matrix_heatmap_with_metrics(train_cm, class_names, train_true, train_pred, \"\\nTraining Confusion Matrix\\n\")  # Training confusion matrix\n",
    "    plot_confusion_matrix_heatmap_with_metrics(val_cm, class_names, val_true, val_pred, \"\\nValidation Confusion Matrix\\n\")  # Validation confusion matrix\n",
    "\n",
    "    display(HTML('<div style=\"text-align:center;\"><strong>Score Report</strong></div>'))\n",
    "    \n",
    "    plot_scores(train_score_avgs, class_names, y_encoded, \"Training\")\n",
    "    plot_scores(val_scores, class_names, y_encoded, \"Validation\")    \n",
    "\n",
    "\n",
    "def accumulate_scores(score_matrix, idx, neighbor_labels, weights_arr, class_indices):\n",
    "    class_weight_sum = defaultdict(float)     \n",
    "    # For each test sample, get the weighted score for each class\n",
    "    for lbl, w in zip(neighbor_labels, weights_arr):\n",
    "        class_weight_sum[lbl] += w\n",
    "    for cls_idx in class_indices:\n",
    "        score_matrix[idx, cls_idx] += class_weight_sum.get(cls_idx, 0.0)\n",
    "\n",
    "# Obtain classification results\n",
    "def run_cross_validation(X, y_encoded, k, metric, weights, class_indices, n):\n",
    "    splitter = LeaveOneOut() if n == 1 else StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "    model = KNeighborsClassifier(n_neighbors=k, metric=metric, weights=map_weights(weights))\n",
    "    n_classes = len(class_indices)\n",
    "\n",
    "    val_true, val_pred = [], []\n",
    "    sample_scores = np.zeros((len(X), n_classes))\n",
    "    train_score_sums = np.zeros((len(X), n_classes))\n",
    "    train_score_counts = np.zeros(len(X))\n",
    "    train_labels = [None] * len(X)\n",
    "\n",
    "    for train_idx, test_idx in splitter.split(X, y_encoded):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        val_preds = model.predict(X_test)\n",
    "        val_pred.extend(val_preds)\n",
    "        val_true.extend(y_test)\n",
    "\n",
    "        neigh_dists_test, neigh_indices_test = model.kneighbors(X_test, n_neighbors=k)\n",
    "        for i, (test_i, neighbors, dists) in enumerate(zip(test_idx, neigh_indices_test, neigh_dists_test)):\n",
    "            weights_arr = get_weight_array(dists, k, weights)\n",
    "            accumulate_scores(sample_scores, test_i, y_train[neighbors], weights_arr, class_indices)\n",
    "\n",
    "        train_preds = model.predict(X_train)\n",
    "        neigh_dists_train, neigh_indices_train = model.kneighbors(X_train, n_neighbors=k)\n",
    "        for i, (train_i, neighbors, dists) in enumerate(zip(train_idx, neigh_indices_train, neigh_dists_train)):\n",
    "            weights_arr = get_weight_array(dists, k, weights)\n",
    "            accumulate_scores(train_score_sums, train_i, y_train[neighbors], weights_arr, class_indices)\n",
    "            train_score_counts[train_i] += 1\n",
    "            train_labels[train_i] = y_encoded[train_i]\n",
    "    return val_true, val_pred, sample_scores, train_score_sums, train_score_counts, train_labels\n",
    "\n",
    "def compute_specificity(cm):\n",
    "    specificity = []\n",
    "    for i in range(len(cm)):  # For each class\n",
    "        TP = cm[i][i]  # True positives for class i\n",
    "        FP = sum(cm[:, i]) - TP  # False positives for class i\n",
    "        FN = sum(cm[i, :]) - TP  # False negatives for class i\n",
    "        TN = cm.sum() - (TP + FP + FN)  # True negatives for class i\n",
    "        denom = TN + FP\n",
    "        specificity.append(TN / denom if denom else 0.0)  # Avoid division by zero\n",
    "    return np.array(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f157a44-7193-4bca-94cb-bfc6ae9a3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix heatmaps for training and validation datasets\n",
    "def plot_confusion_matrix_heatmap_with_metrics(cm, labels, true_labels, pred_labels, title):\n",
    "    total = cm.sum()\n",
    "    n = len(labels)\n",
    "\n",
    "    # Create annotated matrix (n+1)x(n+1)\n",
    "    annotated_cm = np.empty((n + 1, n + 1), dtype=object)\n",
    "\n",
    "    # Fill confusion matrix cells\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            count = cm[i, j]\n",
    "            percent = round(count / total * 100)\n",
    "            annotated_cm[i, j] = f\"{count}/{total}\\n({percent}%)\"\n",
    "\n",
    "    # Add recall (rightmost column)\n",
    "    recalls = recall_score(true_labels, pred_labels, labels=range(n), average=None, zero_division=0)\n",
    "    for i in range(n):\n",
    "        correct = cm[i, i]\n",
    "        total_true = cm[i].sum()\n",
    "        recall_pct = round(recalls[i] * 100)\n",
    "        annotated_cm[i, -1] = f\"{correct}/{total_true if total_true else 1}\\n({recall_pct}%)\"\n",
    "\n",
    "    # Add precision (bottom row)\n",
    "    precisions = precision_score(true_labels, pred_labels, labels=range(n), average=None, zero_division=0)\n",
    "    for j in range(n):\n",
    "        correct = cm[j, j]\n",
    "        total_pred = cm[:, j].sum()\n",
    "        precision_pct = round(precisions[j] * 100)\n",
    "        annotated_cm[-1, j] = f\"{correct}/{total_pred if total_pred else 1}\\n({precision_pct}%)\"\n",
    "\n",
    "    annotated_cm[-1, -1] = \"\"  # bottom-right corner\n",
    "\n",
    "    # Extend the original confusion matrix with zeros to match shape\n",
    "    extended_cm = np.zeros((n + 1, n + 1))\n",
    "    extended_cm[:n, :n] = cm\n",
    "\n",
    "    # Create extended label set\n",
    "    xticklabels = list(labels) + [\"Recall\"]\n",
    "    yticklabels = list(labels) + [\"Precision\"]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(extended_cm, annot=annotated_cm, fmt=\"\", cmap=\"Greens\",\n",
    "                xticklabels=xticklabels, yticklabels=yticklabels, cbar=False,\n",
    "                linewidths=0, linecolor='gray')\n",
    "    ax.xaxis.set_ticks_position('top')       # Move ticks to top\n",
    "\n",
    "    # Determine number of rows and columns\n",
    "    nrows, ncols = extended_cm.shape\n",
    "    \n",
    "    # Highlight last row and last column\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if i == nrows - 1 or j == ncols - 1:\n",
    "                # Calculate patch position: seaborn heatmap uses (col, row) as (x, y)\n",
    "                rect = plt.Rectangle(\n",
    "                    (j, i), 1, 1,\n",
    "                    fill=True,\n",
    "                    facecolor='white',   # Light yellow\n",
    "                    edgecolor='black',\n",
    "                    linewidth=0\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Redraw annotations on top\n",
    "    for t in ax.texts:\n",
    "        t.set_zorder(10)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb880b40-94a9-4d60-a612-ad492f2f22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score plots for training and testing samples\n",
    "def plot_scores (scores, class_names, true_labels,score_set):\n",
    "    n_items = scores.shape[0]\n",
    "    n_classes = len(class_names)  # Can be 2, 3, or more\n",
    "\n",
    "    # Generate distinct colors using a colormap\n",
    "    cmap = plt.colormaps['tab10']  # Or try 'tab20', 'Set3', etc.\n",
    "    class_colors = {i: cmap(i % cmap.N) for i in range(n_classes)}\n",
    "    \n",
    "    # Initialize bottoms for stacking\n",
    "    pos_bottoms = np.zeros(n_items)\n",
    "    neg_bottoms = np.zeros(n_items)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        values = []\n",
    "        bottoms = []\n",
    "        colors = []\n",
    "\n",
    "        for i in range(n_items):\n",
    "            score = scores[i, cls]\n",
    "            is_true_class = (cls == true_labels[i])\n",
    "\n",
    "            if is_true_class:\n",
    "                values.append(score)\n",
    "                bottoms.append(pos_bottoms[i])\n",
    "                pos_bottoms[i] += score  # Accumulate for next class\n",
    "            else:\n",
    "                values.append(-score)\n",
    "                bottoms.append(neg_bottoms[i])\n",
    "                neg_bottoms[i] -= score  # Accumulate downward\n",
    "            colors.append(class_colors[cls])\n",
    "\n",
    "        # Draw bars for this class across all items\n",
    "        plt.bar(range(n_items), values, bottom=bottoms, color=colors, width=0.6)\n",
    "\n",
    "    #print(scores)\n",
    "    plt.xlabel('Item ID')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Scores per Item ({score_set})')\n",
    "    \n",
    "    # Custom legend\n",
    "    legend_elements = [Patch(facecolor=class_colors[i], label=f'{class_names[i]}(Class {i})') for i in range(n_classes)]\n",
    "    plt.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=n_classes)\n",
    "    plt.grid(True, linestyle=':', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7e7ab-b8fa-40bd-94d1-7d801f7f5df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
