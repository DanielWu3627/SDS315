{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f608ca-96c5-44fe-9b5d-40f03b02feb9",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classification\n",
    "\n",
    "This is a user-friendly Jupyter Notebook version of KNN classifier. It allows users to:\n",
    "\n",
    "- Upload a dataset\n",
    "- Perform grid search for the best k with LOOCV\n",
    "- View classification results (accuracy, sensitivity, specificity)\n",
    "- Visualize the confusion matrix for the entire dataset\n",
    "- Visualize score report for each sample    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18962687-0b97-4510-834c-97f2ae2fc109",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b199124c-f210-4a6d-b08e-1762c8e9a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np # Numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV  # Cross-validation strategies\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, auc  \n",
    "from sklearn.preprocessing import label_binarize\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, Dropdown, Button, HBox, VBox, Label, Layout   # For UI Control\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import io  # Provides tools for handling input/output\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af99286-8da1-4ac6-add0-0b3c4e25ff0a",
   "metadata": {},
   "source": [
    "## Interactive UI\n",
    "* Click the \"Upload File\" button and choose the input .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dea6db9-a35f-437a-803c-efef807fe88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Upload CSV File</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2d5d93ccdc41c9b4881ed6acae8baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.csv', description='Upload File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File upload and run button\n",
    "uploader = widgets.FileUpload(accept='.csv', multiple=False, description=\"Upload File\")\n",
    "run_button = widgets.Button(description='Run KNN', button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "display(HTML(\"<b>Upload CSV File</b>\"))\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3238a3-a64a-4d41-96c8-4b1cebd256f8",
   "metadata": {},
   "source": [
    "#### Note: Don't run this cell again after uploading the file! This cell should only be run once when you start the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da71a7-8ca3-4d9c-9347-4c4c2e5161b6",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "Below are helper functions for the notebook. You need run them all before checking the KNN results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd99329-cf7c-46c1-98c1-db5f1c7e57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized function for calculating weights\n",
    "def compute_weights(weight, dist = None, for_sklearn = False):\n",
    "    if for_sklearn:\n",
    "        if weight == 'one':\n",
    "            return 'uniform'\n",
    "        elif weight == 'inverse':\n",
    "            return 'distance'\n",
    "        else:\n",
    "            return lambda d: 1 / (d**2 + 1e-5)\n",
    "\n",
    "    else:\n",
    "        if weight == \"one\":\n",
    "            return np.ones(len(dist))\n",
    "        elif weight == \"inverse\":\n",
    "            return 1 / (dist + 1e-5)\n",
    "        else:\n",
    "            return 1 / (dist**2 + 1e-5)\n",
    "\n",
    "# Calculate scores for each sample\n",
    "def accumulate_scores(score_matrix, idx, neighbor_labels, weights_arr, class_indices):\n",
    "    class_weight_sum = defaultdict(float)     \n",
    "    # For each test sample, get the weighted score for each class\n",
    "    for lbl, w in zip(neighbor_labels, weights_arr):\n",
    "        class_weight_sum[lbl] += w\n",
    "    for cls_idx in class_indices:\n",
    "        score_matrix[idx, cls_idx] += class_weight_sum.get(cls_idx, 0.0)\n",
    "\n",
    "\n",
    "# Calculate specificity\n",
    "def compute_specificity(cm):\n",
    "    specificity = []\n",
    "    for i in range(len(cm)):  # For each class\n",
    "        TP = cm[i][i]  # True positives for class i\n",
    "        FP = sum(cm[:, i]) - TP  # False positives for class i\n",
    "        FN = sum(cm[i, :]) - TP  # False negatives for class i\n",
    "        TN = cm.sum() - (TP + FP + FN)  # True negatives for class i\n",
    "        denom = TN + FP\n",
    "        specificity.append(TN / denom if denom else 0.0)  # Avoid division by zero\n",
    "    return np.array(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f39fe0-ffe8-4fd1-9f61-f5843feca868",
   "metadata": {},
   "source": [
    "The function below evaluates the final KNN model on the entire dataset, using the best K from grid search. It first reports overall accuracy, then sensitivity, selectivity (specificity), and non-error rate for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a10d8e-b553-42ff-b32a-f114b68771ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_final(X, y, k, metric, weights):\n",
    "    global train_true, train_pred, train_cm, sample_scores, model\n",
    "    \n",
    "    # Final Model Evaluation on Full Dataset\n",
    "    class_indices = list(range(len(class_names)))\n",
    "    model = KNeighborsClassifier(n_neighbors=k, metric=metric, weights=compute_weights(weights, for_sklearn=True))\n",
    "    model.fit(X, y_encoded)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Store results\n",
    "    train_true = y_encoded\n",
    "    train_pred = y_pred\n",
    "    train_cm = confusion_matrix(train_true, train_pred, labels=class_indices)\n",
    "    train_acc = accuracy_score(train_true, train_pred)\n",
    "\n",
    "    # Calculate scores \n",
    "    n_classes = len(set(y_encoded))\n",
    "    sample_scores = np.zeros((len(X), n_classes))\n",
    "\n",
    "    # Get neighbors of each sample (including self)\n",
    "    neigh_dists, neigh_indices = model.kneighbors(X, n_neighbors=k)\n",
    "\n",
    "    # Compute scores for each sample\n",
    "    for i in range(len(X)):\n",
    "        neighbors = neigh_indices[i]\n",
    "        dists = neigh_dists[i]\n",
    "        weights_arr = compute_weights(weights, dists)\n",
    "        neighbor_labels = y_encoded[neighbors]\n",
    "        accumulate_scores(sample_scores, i, neighbor_labels, weights_arr, class_indices)\n",
    " \n",
    "    \n",
    "    display(HTML('<div style=\"text-align:left;\"><strong>Summary Report for the Full Dataset</strong></div>'))\n",
    "\n",
    "    print(f\"\\nRunning Final KNN with k={k}\")\n",
    "    print(\"Model trained and evaluated on the full dataset\")\n",
    "    print(f\"Input file name: {file_name}\\n\")\n",
    "    \n",
    "    print(f\"Overall Accuracy\\t{train_acc * 100:.2f}% ({int(train_acc * len(train_true))}/{len(train_true)})\")\n",
    "   \n",
    "\n",
    "    train_recall = recall_score(train_true, train_pred, labels=class_indices, average=None, zero_division=0)\n",
    "    train_specificity = compute_specificity(train_cm)\n",
    "\n",
    "    for i, cls in enumerate(class_names):\n",
    "        train_sens = train_recall[i] * 100  \n",
    "        train_spec = train_specificity[i] * 100\n",
    "        train_ner = (train_recall[i] + train_specificity[i]) / 2 * 100\n",
    "        \n",
    "        tp_train = train_cm[i][i]\n",
    "        fn_train = train_cm[i, :].sum() - tp_train\n",
    "        fp_train = train_cm[:, i].sum() - tp_train\n",
    "        tn_train = train_cm.sum() - (tp_train + fn_train + fp_train)\n",
    "\n",
    "        print(\"\")\n",
    "        \n",
    "        print(f\"{cls:<10}{'Sensitivity':>20}{'Selectivity':>25}{'Non-Error Rate':>25}\")\n",
    "        print(f\"{' ':<10}{train_sens:>14.2f}% ({tp_train}/{tp_train+fn_train}){train_spec:>17.2f}% ({tn_train}/{tn_train+fp_train}){train_ner:>21.2f}%\")\n",
    "       \n",
    "    print(\"\\n\\n\")\n",
    "    return train_cm, class_names, train_true, train_pred, sample_scores, y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e79862-d2a4-46f6-964a-d1bbe7f3fc3e",
   "metadata": {},
   "source": [
    "These two functions perform grid search over a range of K values (1 to 20) for a KNN classifier using Leave-One-Out Cross-Validation (LOOCV). Both functions search for the optimal number of neighbors (k) that maximizes model performance, but run_grid_accuracy() optimizes for the overall accuracy, while run_grid_roc() optimizes for AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cfcaf70-b184-4e3f-92f5-7ad71d16992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_accuracy(X, y, metric, weights):\n",
    "    global class_names, y_encoded, class_indices\n",
    "    class_names = list(dict.fromkeys(y))\n",
    "    label_to_index = {label: idx for idx, label in enumerate(class_names)}\n",
    "    y_encoded = np.array([label_to_index[label] for label in y])\n",
    "    class_indices = list(range(len(class_names)))\n",
    "    \n",
    "    # === Grid Search for best K ===\n",
    "    param_grid = {'n_neighbors': list(range(1, 21))}\n",
    "    grid = GridSearchCV(\n",
    "        KNeighborsClassifier(metric=metric, weights=compute_weights(weights, for_sklearn=True)),\n",
    "        param_grid,\n",
    "        cv=LeaveOneOut()    \n",
    "    )\n",
    "   \n",
    "    # Time the grid search\n",
    "    start = time.time()\n",
    "    grid.fit(X, y)\n",
    "    end = time.time()\n",
    "    \n",
    "    # Sort by mean_test_score descending\n",
    "    grid_scores = grid.cv_results_\n",
    "    n_top = 10\n",
    "    results = []\n",
    "    for i in range(len(grid_scores['params'])):\n",
    "        mean = grid_scores['mean_test_score'][i]\n",
    "        std = grid_scores['std_test_score'][i]\n",
    "        params = grid_scores['params'][i]\n",
    "        results.append((mean, std, params))\n",
    "\n",
    "    results = sorted(results, key=lambda x: -x[0])\n",
    "    \n",
    "    print(f\"GridSearchCV took {end - start:.2f} seconds for {len(results)} candidate parameter settings.\")\n",
    "    \n",
    "    for rank, (mean, std, params) in enumerate(results[:n_top], start=1):\n",
    "        print(f\"\\nModel with rank {rank}:\")\n",
    "        print(f\"Mean validation score: {mean:.3f} (std: {std:.3f})\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "\n",
    "    # Display results for best k\n",
    "    best_k = grid.best_params_['n_neighbors']\n",
    "    best_score = grid.best_score_\n",
    "    print(f\"\\nBest K: {best_k} (Mean LOOCV Accuracy: {best_score * 100:.2f}%)\\n\")\n",
    "    evaluate_final(X, y, best_k, metric, weights)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def run_grid_roc(X, y, metric, weights):\n",
    "    global class_names, y_encoded, class_indices\n",
    "\n",
    "    class_names = list(dict.fromkeys(y))\n",
    "    label_to_index = {label: idx for idx, label in enumerate(class_names)}\n",
    "    y_encoded = np.array([label_to_index[label] for label in y])\n",
    "    class_indices = list(range(len(class_names)))\n",
    "    n_classes = len(class_indices)\n",
    "\n",
    "    k_values = list(range(1, 21))\n",
    "    auc_scores = []\n",
    "    prob_dict = {}  # store probs per k for top-3 ROC plot\n",
    "    truth_dict = {}\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for k in k_values:\n",
    "        probs = []\n",
    "        truths = []\n",
    "\n",
    "        loo = LeaveOneOut()\n",
    "        for train_index, test_index in loo.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "            model = KNeighborsClassifier(\n",
    "                n_neighbors=k,\n",
    "                metric=metric,\n",
    "                weights=compute_weights(weights, for_sklearn=True)\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            proba = model.predict_proba(X_test)[0]\n",
    "            probs.append(proba)\n",
    "            truths.append(y_test[0])\n",
    "\n",
    "        if n_classes == 2:\n",
    "            auc = roc_auc_score(truths, [p[1] for p in probs])\n",
    "        else:\n",
    "            auc = roc_auc_score(truths, probs, multi_class='ovr', average='macro')\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        prob_dict[k] = np.array(probs)\n",
    "        truth_dict[k] = np.array(truths)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Reporting top results\n",
    "    results = [(auc_scores[i], 0.0, {'n_neighbors': k_values[i]}) for i in range(len(k_values))]\n",
    "    results = sorted(results, key=lambda x: -x[0])\n",
    "    print(f\"Custom ROC Grid Search took {end - start:.2f} seconds for {len(results)} candidate parameter settings.\")\n",
    "\n",
    "    n_top = 3\n",
    "    for rank, (mean, std, params) in enumerate(results[:n_top], start=1):\n",
    "        print(f\"\\nModel with rank {rank}:\")\n",
    "        print(f\"Mean validation AUC: {mean:.4f}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "\n",
    "    # === Plot macro-average ROC curves for top 3 K ===\n",
    "    print(\"\\nPlotting macro-average ROC curves for top 3 K values...\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for _, _, param in results[:n_top]:\n",
    "        k = param['n_neighbors']\n",
    "        probs = prob_dict[k]\n",
    "        truths = truth_dict[k]\n",
    "\n",
    "        y_true_bin = label_binarize(truths, classes=class_indices)\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        for i in class_indices:\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], probs[:, i])\n",
    "        \n",
    "        # Compute macro-average ROC\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in class_indices]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in class_indices:\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes\n",
    "\n",
    "        plt.plot(all_fpr, mean_tpr, label=f'K={k} (AUC={auc_scores[k - 1]:.3f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves for Top 3 K\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Final model on full dataset\n",
    "    best_k = results[0][2]['n_neighbors']\n",
    "    best_auc = results[0][0]\n",
    "    print(f\"\\nBest K: {best_k} (Mean LOOCV AUC: {best_auc:.4f})\\n\")\n",
    "    evaluate_final(X, y, best_k, metric, weights)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8870a-c302-428b-bacb-a1b207a6ef78",
   "metadata": {},
   "source": [
    "#### Make sure you have uploaded the input file, then run the code below. \n",
    "Use the buttons below to run KNN with different grid search strategies:\n",
    "* Grid Search (default)\n",
    "Selects the best number of neighbors (K) using Leave-One-Out Cross-Validation based on mean accuracy.\n",
    "Recommended for general classification tasks.\n",
    "* Grid Search (ROC)\n",
    "Selects the best K based on mean AUC (Area Under the ROC Curve) across LOOCV folds.\n",
    "Recommended when your goal is to optimize ROC performance, especially with imbalanced or multiclass datasets.\n",
    "\n",
    "After selection, a final KNN model will be trained and evaluated on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "632bc107-5a03-461d-8d43-ddb30ba11fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74231453c16148209af9475d00f125d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Grid Search (default)', style=ButtonStyle()), Butto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ce6003ac0f4657a665cef032a8e55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_file_content():\n",
    "    with output:  # All printed output will appear inside the 'output' widget\n",
    "        output.clear_output()  # Clear previous output\n",
    "        if not uploader.value:\n",
    "            print(\"⚠️ No file uploaded. Please choose the input file and re-run the code above.\")\n",
    "            return False\n",
    "        else:\n",
    "            global file_name, X, y\n",
    "            value = uploader.value\n",
    "            \n",
    "            # Handle multiple possible structures: dict, tuple, list\n",
    "            if isinstance(value, dict):\n",
    "                uploaded_file = list(value.values())[0]\n",
    "            elif isinstance(value, (list, tuple)):\n",
    "                uploaded_file = value[0]\n",
    "            else:\n",
    "                print(\"Unsupported uploader value format.\")\n",
    "                return False\n",
    "            content = uploaded_file['content']                # Binary content of the file\n",
    "            file_name = uploaded_file['name']        \n",
    "            try:\n",
    "                data = pd.read_csv(io.BytesIO(content))     # Read into a DataFrame from memory buffer\n",
    "                X = data.drop(columns=['NAME', 'CLASS'])    # Use all columns except 'NAME' and 'CLASS' as features\n",
    "                y = data['CLASS']                           # Use 'CLASS' as the target label\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                # Catch and display any error that occurs during file read or processing\n",
    "                print(f\"Error: {e}\")\n",
    "                return False\n",
    "            \n",
    "def on_accuracy_clicked(b):  \n",
    "    if get_file_content():\n",
    "        with output:\n",
    "            run_grid_accuracy(X, y, metric=\"euclidean\", weights=\"one\")\n",
    "\n",
    "def on_roc_clicked(b):\n",
    "    if get_file_content():\n",
    "        with output:\n",
    "            run_grid_roc(X, y, metric=\"euclidean\", weights=\"one\")  # Or use your widget values here\n",
    "\n",
    "output = widgets.Output()\n",
    "output.clear_output()\n",
    "\n",
    "# Run button\n",
    "accuracy_button = widgets.Button(description='Grid Search (default)', button_style='success')\n",
    "roc_button = widgets.Button(description='Grid Search (ROC)', button_style='info')\n",
    "\n",
    "accuracy_button.on_click(on_accuracy_clicked)\n",
    "roc_button.on_click(on_roc_clicked)\n",
    "\n",
    "\n",
    "# === Final Output Display ===\n",
    "display(\n",
    "    widgets.HBox([accuracy_button, roc_button]),\n",
    "    output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a31e7b-7544-4286-8a01-60645adb526e",
   "metadata": {},
   "source": [
    "#### This function plots the ROC curves for each class using the final KNN model trained on the entire dataset. \n",
    "The model uses the best K value selected from either grid search strategy (accuracy or AUC-based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de44ea-1f65-4df8-9e7f-7b4f44f50b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc_curve_knn(model, X, y_true, class_names):\n",
    "    n_classes = len(class_names)\n",
    "    y_score = model.predict_proba(X)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    if n_classes == 2:\n",
    "        # Binary classification\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", color='blue')\n",
    "        title = 'ROC Curve (Binary)'\n",
    "    else:\n",
    "        # Multiclass classification\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
    "        title = 'ROC Curve (Multiclass)'\n",
    "\n",
    "    # Common plot formatting\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve_knn(model, X, y_encoded, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ac718-319a-4406-866b-3be98e0c352a",
   "metadata": {},
   "source": [
    "####  Run the cell below to plot confusion matrix heatmaps for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f157a44-7193-4bca-94cb-bfc6ae9a3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_heatmap_with_metrics(cm, labels, true_labels, pred_labels, title):\n",
    "    total = cm.sum()\n",
    "    n = len(labels)\n",
    "\n",
    "    # Create annotated matrix (n+1)x(n+1)\n",
    "    annotated_cm = np.empty((n + 1, n + 1), dtype=object)\n",
    "\n",
    "    # Fill confusion matrix cells\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            count = cm[i, j]\n",
    "            percent = round(count / total * 100)\n",
    "            annotated_cm[i, j] = f\"{count}/{total}\\n({percent}%)\"\n",
    "\n",
    "    # Add recall (rightmost column)\n",
    "    recalls = recall_score(true_labels, pred_labels, labels=range(n), average=None, zero_division=0)\n",
    "    for i in range(n):\n",
    "        correct = cm[i, i]\n",
    "        total_true = cm[i].sum()\n",
    "        recall_pct = round(recalls[i] * 100)\n",
    "        annotated_cm[i, -1] = f\"{correct}/{total_true if total_true else 1}\\n({recall_pct}%)\"\n",
    "\n",
    "    # Add precision (bottom row)\n",
    "    precisions = precision_score(true_labels, pred_labels, labels=range(n), average=None, zero_division=0)\n",
    "    for j in range(n):\n",
    "        correct = cm[j, j]\n",
    "        total_pred = cm[:, j].sum()\n",
    "        precision_pct = round(precisions[j] * 100)\n",
    "        annotated_cm[-1, j] = f\"{correct}/{total_pred if total_pred else 1}\\n({precision_pct}%)\"\n",
    "\n",
    "    annotated_cm[-1, -1] = \"\"  # bottom-right corner\n",
    "\n",
    "    # Extend the original confusion matrix with zeros to match shape\n",
    "    extended_cm = np.zeros((n + 1, n + 1))\n",
    "    extended_cm[:n, :n] = cm\n",
    "\n",
    "    # Create extended label set\n",
    "    xticklabels = list(labels) + [\"Recall\"]\n",
    "    yticklabels = list(labels) + [\"Precision\"]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(extended_cm, annot=annotated_cm, fmt=\"\", cmap=\"Greens\",\n",
    "                xticklabels=xticklabels, yticklabels=yticklabels, cbar=False,\n",
    "                linewidths=0, linecolor='gray')\n",
    "    ax.xaxis.set_ticks_position('top')       # Move ticks to top\n",
    "\n",
    "    # Determine number of rows and columns\n",
    "    nrows, ncols = extended_cm.shape\n",
    "    \n",
    "    # Highlight last row and last column\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if i == nrows - 1 or j == ncols - 1:\n",
    "                # Calculate patch position: seaborn heatmap uses (col, row) as (x, y)\n",
    "                rect = plt.Rectangle(\n",
    "                    (j, i), 1, 1,\n",
    "                    fill=True,\n",
    "                    facecolor='white',   # Light yellow\n",
    "                    edgecolor='black',\n",
    "                    linewidth=0\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Redraw annotations on top\n",
    "    for t in ax.texts:\n",
    "        t.set_zorder(10)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix_heatmap_with_metrics(train_cm, class_names, train_true, train_pred, \"\\nOverall Confusion Matrix\\n\")  # Training confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fa5be-5b33-4822-b144-c841a87857c2",
   "metadata": {},
   "source": [
    "#### Run the code below to plot scores for each item in the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb880b40-94a9-4d60-a612-ad492f2f22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score plots for training and testing samples\n",
    "def plot_scores (scores, class_names, true_labels,score_set):\n",
    "    n_items = scores.shape[0]\n",
    "    n_classes = len(class_names)  # Can be 2, 3, or more\n",
    "\n",
    "    # Generate distinct colors using a colormap\n",
    "    cmap = plt.colormaps['tab10']  # Or try 'tab20', 'Set3', etc.\n",
    "    class_colors = {i: cmap(i % cmap.N) for i in range(n_classes)}\n",
    "    \n",
    "    # Initialize bar positions for stacking\n",
    "    pos_bottoms = np.zeros(n_items)\n",
    "    neg_bottoms = np.zeros(n_items)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        values = []\n",
    "        bottoms = []\n",
    "        colors = []\n",
    "\n",
    "        for i in range(n_items):\n",
    "            score = scores[i, cls]\n",
    "            is_true_class = (cls == true_labels[i])\n",
    "\n",
    "            if is_true_class:\n",
    "                values.append(score)\n",
    "                bottoms.append(pos_bottoms[i])\n",
    "                pos_bottoms[i] += score  # Accumulate for next class\n",
    "            else:\n",
    "                values.append(-score)\n",
    "                bottoms.append(neg_bottoms[i])\n",
    "                neg_bottoms[i] -= score  # Accumulate downward\n",
    "            colors.append(class_colors[cls])\n",
    "\n",
    "        # Draw bars for this class across all items\n",
    "        plt.bar(range(n_items), values, bottom=bottoms, color=colors, width=0.6)\n",
    "\n",
    "    #print(scores)\n",
    "    plt.xlabel('Item ID')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Scores per Item ({score_set})')\n",
    "    \n",
    "    # Custom legend\n",
    "    legend_elements = [Patch(facecolor=class_colors[i], label=f'{class_names[i]}(Class {i})') for i in range(n_classes)]\n",
    "    plt.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=n_classes)\n",
    "    plt.grid(True, linestyle=':', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display(HTML('<div style=\"text-align:center;\"><strong>Score Report</strong></div>'))\n",
    "plot_scores(sample_scores, class_names, y_encoded, \"(All Data)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KNN_env",
   "language": "python",
   "name": "gc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
